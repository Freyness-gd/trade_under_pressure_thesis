{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BasicLSTM",
   "id": "a1d433415f0426aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation",
   "id": "e8067785cd8d7919"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import modules",
   "id": "1fe72819da94dd5d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:16.326190Z",
     "start_time": "2025-05-22T13:24:16.323187Z"
    }
   },
   "source": [
    "# Prediction using LSTM, GRU-LSTM, xLSTM\n",
    "import copy\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import thesis_utils.datastruc as tuds\n",
    "import thesis_utils.models as tumod"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "4284e3ddbaa79d6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:16.332643Z",
     "start_time": "2025-05-22T13:24:16.328628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Config for saving outputs\n",
    "SAVE_ENABLED = True\n",
    "SERIAL_NUMBER = \"NOT_SET\"\n",
    "\n",
    "# Model parameters\n",
    "SEQ_LEN = 5\n",
    "HORIZON = 1\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "HIDDEN_SIZE = 128\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Train parameters\n",
    "TARGET = \"gravity_trade\"\n",
    "FEATURES = [\n",
    "  \"GDP_reporter\",\n",
    "  \"GDP_partner\",\n",
    "  \"distw\",\n",
    "  \"TOTAL\",\n",
    "  \"arms\", \"military\", \"trade\", \"financial\", \"travel\", \"other\",  # sanctions categorical\n",
    "  \"contig\", \"comlang_off\", \"colony\", \"smctry\",  # dist cepii categorical\n",
    "  \"fyear\", \"GDP_yearly_average\"  # additional features\n",
    "]\n",
    "N_SPLITS = 3\n",
    "PATIENCE = 15\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 0.01\n",
    "RANDOM_SEED = 16\n",
    "KEEP_FRAC = 1\n",
    "\n",
    "# Torch config\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = (\n",
    "  torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "  else torch.device(\"cpu\")\n",
    ")"
   ],
   "id": "56e9b02ef240fd0a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "c9e963c3ab4f2220"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:16.716113Z",
     "start_time": "2025-05-22T13:24:16.346033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed = pa.read_parquet(path=\"../data/model/processed.parquet\", engine=\"fastparquet\")\n",
    "df: DataFrame = processed.copy(deep=True)"
   ],
   "id": "9451e98a113f268d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sort, shift and compute data",
   "id": "84523341884506e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.003383Z",
     "start_time": "2025-05-22T13:24:16.730172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sort data by Report + Partner + Year\n",
    "df[\"dyad_id\"] = df[\"ISO3_reporter\"] + \"_\" + df[\"ISO3_partner\"]\n",
    "df = df.sort_values(by=[\"dyad_id\", \"Year\"], ignore_index=True)"
   ],
   "id": "1dcefe0333d9c64b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.028839Z",
     "start_time": "2025-05-22T13:24:17.016902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add gravity_trade as value column\n",
    "df[\"gravity_trade\"] = np.log1p((df[\"GDP_reporter\"] * df[\"GDP_partner\"]) / df[\"distw\"])\n",
    "df[\"TOTAL\"] = df[\"IMPORT\"] + df[\"EXPORT\"]\n",
    "\n",
    "# Add year feature\n",
    "df[\"fyear\"] = df[\"Year\"]"
   ],
   "id": "80779ed2ae23e3de",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalize",
   "id": "d5984693829da5ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.246903Z",
     "start_time": "2025-05-22T13:24:17.042551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale data\n",
    "scale_columns_minmax = [\"GDP_reporter\", \"GDP_partner\", \"TOTAL\", \"fyear\", \"GDP_yearly_average\"]\n",
    "scaler_rb = RobustScaler()\n",
    "scaler_mm = MinMaxScaler()\n",
    "df_scaled: DataFrame = df.copy(deep=True)\n",
    "df_scaled[scale_columns_minmax] = scaler_mm.fit_transform(df[scale_columns_minmax])\n",
    "df_scaled[[\"distw\"]] = scaler_rb.fit_transform(df[[\"distw\"]])"
   ],
   "id": "77dd6217a0e1c4a0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split data",
   "id": "eab345a3bd4fc194"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.297199Z",
     "start_time": "2025-05-22T13:24:17.261741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split into Train, Validation and Test sets\n",
    "idx = np.arange(len(df_scaled))\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "  idx, test_size=0.20, random_state=RANDOM_SEED\n",
    ")\n",
    "train_idx, val_idx = train_test_split(\n",
    "  train_idx, test_size=20, random_state=RANDOM_SEED\n",
    ")"
   ],
   "id": "51433a3f014528c2",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train",
   "id": "1053e63e8a6ccb76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Fold and Epoch steps\n",
    "_For reusability_"
   ],
   "id": "78e3575a51038c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.312457Z",
     "start_time": "2025-05-22T13:24:17.310636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create KFold object\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)"
   ],
   "id": "8eb0c16ea3f9a15d",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.328230Z",
     "start_time": "2025-05-22T13:24:17.325466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define epoch step\n",
    "def epoch_step(model: nn.Module, optimizer: Optimizer, criterion: nn.Module,\n",
    "               scheduler: LRScheduler, train_loader: DataLoader, val_loader: DataLoader,\n",
    "               device: any) -> float:\n",
    "  model.train()\n",
    "  for X, y, _ in train_loader:\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(model(X), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  val_losses = []\n",
    "  with (torch.no_grad()):\n",
    "    for X, y, _ in val_loader:\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      val_losses.append(criterion(model(X), y).item())\n",
    "\n",
    "  val_rmse = math.sqrt((sum(val_losses) / len(val_losses)))\n",
    "  scheduler.step(val_rmse)\n",
    "  return val_rmse"
   ],
   "id": "82612d122ae0a3ae",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.346035Z",
     "start_time": "2025-05-22T13:24:17.341807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define fold step\n",
    "def fold_step(fold: int, train_idx: List, val_idx: List,\n",
    "              dataset: Dataset, batch_size: int, num_epochs: int, patience: int,\n",
    "              model: nn.Module, device: any,\n",
    "              optimizer: Optimizer, criterion: nn.Module, scheduler: LRScheduler) -> (float, dict):\n",
    "  train_loader = DataLoader(\n",
    "    Subset(dataset, train_idx),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  val_loader = DataLoader(\n",
    "    Subset(dataset, val_idx),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "  )\n",
    "\n",
    "  best_state = copy.deepcopy(model.state_dict())\n",
    "  best_rmse = float(\"inf\")\n",
    "  patience_left = patience\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    val_rmse = epoch_step(model=model, optimizer=optimizer, criterion=criterion,\n",
    "                          scheduler=scheduler, train_loader=train_loader, val_loader=val_loader,\n",
    "                          device=device)\n",
    "    print(f\"Epoch {epoch + 1:02d}/{num_epochs}  |  val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    if val_rmse < best_rmse - 1e-4:\n",
    "      best_rmse, patience_left = val_rmse, 10\n",
    "      best_state = model.state_dict()\n",
    "    else:\n",
    "      patience_left -= 1\n",
    "      if patience_left == 0:\n",
    "        print(\"Early stop.\")\n",
    "        break\n",
    "  model.load_state_dict(best_state)\n",
    "  model.eval()\n",
    "  preds, truth = [], []\n",
    "  with torch.no_grad():\n",
    "    for X, y, _ in val_loader:\n",
    "      X = X.to(device)\n",
    "      preds.append(model(X).cpu())\n",
    "      truth.append(y)\n",
    "  preds = torch.cat(preds).numpy()\n",
    "  truth = torch.cat(truth).numpy()\n",
    "\n",
    "  rmse = np.sqrt(((preds - truth) ** 2).mean())\n",
    "  mae = np.abs(preds - truth).mean()\n",
    "  r2 = 1 - ((preds - truth) ** 2).sum() / ((truth - truth.mean()) ** 2).sum()\n",
    "  print(f\" Fold {fold}  RMSE {rmse:.4f} | MAE {mae:.4f} | RÂ² {r2:.4f}\")\n",
    "\n",
    "  return rmse, copy.deepcopy(best_state)\n"
   ],
   "id": "959f98487abc0673",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Raw dataset",
   "id": "13f3e7e01662654"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split dataset",
   "id": "ff153528228b1350"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.588878Z",
     "start_time": "2025-05-22T13:24:17.359159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert df_scaled to pytorch Tensor\n",
    "dataset, _ = tuds.make_panel_datasets(data=df_scaled, features=FEATURES, target=TARGET, horizon=HORIZON,\n",
    "                                      keep_frac=KEEP_FRAC)"
   ],
   "id": "e38f7e2869374fc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape original data:  (1099125, 32)\n",
      "Shape sampled:  (1099125, 32)\n",
      "Shape remainder:  (0, 32)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.605580Z",
     "start_time": "2025-05-22T13:24:17.602893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoaders for the 3 sets\n",
    "train_loader = DataLoader(\n",
    "  Subset(dataset, train_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  Subset(dataset, val_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "  Subset(dataset, test_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")"
   ],
   "id": "14dd8fdd80d8f0aa",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model",
   "id": "d9e348e74bd91747"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.620846Z",
     "start_time": "2025-05-22T13:24:17.618981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save config\n",
    "SAVE_ENABLED = False\n",
    "SERIAL_NUMBER = f\"BasicLSTM-RawData\""
   ],
   "id": "1b837c71b6de51c1",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:17.636202Z",
     "start_time": "2025-05-22T13:24:17.634357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save best train iteration\n",
    "best_fold_state = None\n",
    "best_fold_rmse = float(\"inf\")"
   ],
   "id": "5e6e1bf43de12079",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-22T13:24:17.649194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset))), 1):\n",
    "  model = tumod.BasicLSTM(\n",
    "    n_features=len(FEATURES),\n",
    "    n_layers=N_LAYERS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    horizon=HORIZON).to(device=device)\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=PATIENCE\n",
    "  )\n",
    "\n",
    "  print(f\"=== FOLD {fold}/{N_SPLITS} ===\")\n",
    "  fold_rmse, best_state = fold_step(fold=fold,\n",
    "                                    train_idx=train_idx,\n",
    "                                    val_idx=val_idx,\n",
    "                                    dataset=dataset,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_epochs=NUM_EPOCHS,\n",
    "                                    patience=PATIENCE,\n",
    "                                    model=model,\n",
    "                                    device=device,\n",
    "                                    optimizer=optimizer,\n",
    "                                    criterion=criterion,\n",
    "                                    scheduler=scheduler)\n",
    "  if fold_rmse < best_fold_rmse:\n",
    "    best_fold_rmse = fold_rmse\n",
    "    best_fold_state = copy.deepcopy(best_state)"
   ],
   "id": "46a31ee0542aa6ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD 1/3 ===\n",
      "Epoch 01/20  |  val RMSE: 2.1003\n",
      "Epoch 02/20  |  val RMSE: 1.9878\n",
      "Epoch 03/20  |  val RMSE: 1.9284\n",
      "Epoch 04/20  |  val RMSE: 1.9367\n",
      "Epoch 05/20  |  val RMSE: 1.9145\n",
      "Epoch 06/20  |  val RMSE: 1.8341\n",
      "Epoch 07/20  |  val RMSE: 1.8230\n",
      "Epoch 08/20  |  val RMSE: 1.8020\n",
      "Epoch 09/20  |  val RMSE: 1.7462\n",
      "Epoch 10/20  |  val RMSE: 1.6539\n",
      "Epoch 11/20  |  val RMSE: 1.6560\n",
      "Epoch 12/20  |  val RMSE: 1.6162\n",
      "Epoch 13/20  |  val RMSE: 1.5676\n",
      "Epoch 14/20  |  val RMSE: 1.5470\n",
      "Epoch 15/20  |  val RMSE: 1.5723\n",
      "Epoch 16/20  |  val RMSE: 1.4795\n",
      "Epoch 17/20  |  val RMSE: 1.5553\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Lagged features",
   "id": "16acca6c1bd49c45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split dataset and lag features",
   "id": "9a80e8e906b1ccaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.121399Z",
     "start_time": "2025-05-22T13:08:09.522958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert df_scaled to pytorch Tensor\n",
    "LAGGED_COLS = [\"GDP_partner\", \"GDP_reporter\", \"TOTAL\"]\n",
    "LAG = 3\n",
    "dataset, _ = tuds.make_panel_laggedsets(data=df_scaled, features=FEATURES, target=TARGET,\n",
    "                                        lag=LAG, lag_columns=LAGGED_COLS, keep_frac=KEEP_FRAC)"
   ],
   "id": "b5ab62978237f26e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape original data:  (1099125, 32)\n",
      "Shape sampled:  (1099125, 32)\n",
      "Shape remainder:  (0, 32)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.133329Z",
     "start_time": "2025-05-22T13:08:10.555075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoaders for the 3 sets\n",
    "train_loader = DataLoader(\n",
    "  Subset(dataset, train_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  Subset(dataset, val_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "  Subset(dataset, test_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_workers=4\n",
    ")"
   ],
   "id": "887f1be35155633c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model",
   "id": "380a10bb22163818"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.149993Z",
     "start_time": "2025-05-22T13:08:10.585407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save config\n",
    "SAVE_ENABLED = False\n",
    "SERIAL_NUMBER = f\"BasicLSTM-LaggedFeatures{LAG}\""
   ],
   "id": "583f768f40ee7362",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.166597Z",
     "start_time": "2025-05-22T13:08:10.589748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save best train iteration\n",
    "best_fold_state = None\n",
    "best_fold_rmse = float(\"inf\")"
   ],
   "id": "5f8c19f77b7a9182",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.181508Z",
     "start_time": "2025-05-22T13:08:10.617705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add lagged feature names\n",
    "LAGGED_FEATURES = copy.deepcopy(FEATURES)\n",
    "for feature in LAGGED_COLS:\n",
    "  for i in range(1, LAG + 1):\n",
    "    LAGGED_FEATURES.append(f\"{feature}_lag{i}\")"
   ],
   "id": "6ecdba98a0ae30f5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.181957Z",
     "start_time": "2025-05-22T13:08:10.624326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset))), 1):\n",
    "  print(f\"=== FOLD {fold}/{N_SPLITS} ===\")\n",
    "  model = tumod.BasicLSTM(\n",
    "    n_features=len(LAGGED_FEATURES),\n",
    "    n_layers=N_LAYERS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    horizon=HORIZON).to(device=device)\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=PATIENCE\n",
    "  )\n",
    "\n",
    "  fold_rmse, best_state = fold_step(fold=fold,\n",
    "                                    train_idx=train_idx,\n",
    "                                    val_idx=val_idx,\n",
    "                                    dataset=dataset,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_epochs=NUM_EPOCHS,\n",
    "                                    patience=PATIENCE,\n",
    "                                    model=model,\n",
    "                                    device=device,\n",
    "                                    optimizer=optimizer,\n",
    "                                    criterion=criterion,\n",
    "                                    scheduler=scheduler)\n",
    "  if fold_rmse < best_fold_rmse:\n",
    "    best_fold_rmse = fold_rmse\n",
    "    best_fold_state = copy.deepcopy(best_state)"
   ],
   "id": "2fe1bf848936771e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD 1/3 ===\n",
      "Epoch 01/20  |  val RMSE: 1.8567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE, weight_decay\u001B[38;5;241m=\u001B[39mWEIGHT_DECAY)\n\u001B[1;32m     11\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mReduceLROnPlateau(\n\u001B[1;32m     12\u001B[0m   optimizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, patience\u001B[38;5;241m=\u001B[39mPATIENCE\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m fold_rmse, best_state \u001B[38;5;241m=\u001B[39m fold_step(fold\u001B[38;5;241m=\u001B[39mfold,\n\u001B[1;32m     16\u001B[0m                                   train_idx\u001B[38;5;241m=\u001B[39mtrain_idx,\n\u001B[1;32m     17\u001B[0m                                   val_idx\u001B[38;5;241m=\u001B[39mval_idx,\n\u001B[1;32m     18\u001B[0m                                   dataset\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[1;32m     19\u001B[0m                                   batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE,\n\u001B[1;32m     20\u001B[0m                                   num_epochs\u001B[38;5;241m=\u001B[39mNUM_EPOCHS,\n\u001B[1;32m     21\u001B[0m                                   patience\u001B[38;5;241m=\u001B[39mPATIENCE,\n\u001B[1;32m     22\u001B[0m                                   model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     23\u001B[0m                                   device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     24\u001B[0m                                   optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[1;32m     25\u001B[0m                                   criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[1;32m     26\u001B[0m                                   scheduler\u001B[38;5;241m=\u001B[39mscheduler)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fold_rmse \u001B[38;5;241m<\u001B[39m best_fold_rmse:\n\u001B[1;32m     28\u001B[0m   best_fold_rmse \u001B[38;5;241m=\u001B[39m fold_rmse\n",
      "Cell \u001B[0;32mIn[10], line 23\u001B[0m, in \u001B[0;36mfold_step\u001B[0;34m(fold, train_idx, val_idx, dataset, batch_size, num_epochs, patience, model, device, optimizer, criterion, scheduler)\u001B[0m\n\u001B[1;32m     20\u001B[0m patience_left \u001B[38;5;241m=\u001B[39m patience\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 23\u001B[0m   val_rmse \u001B[38;5;241m=\u001B[39m epoch_step(model\u001B[38;5;241m=\u001B[39mmodel, optimizer\u001B[38;5;241m=\u001B[39moptimizer, criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[1;32m     24\u001B[0m                         scheduler\u001B[38;5;241m=\u001B[39mscheduler, train_loader\u001B[38;5;241m=\u001B[39mtrain_loader, val_loader\u001B[38;5;241m=\u001B[39mval_loader,\n\u001B[1;32m     25\u001B[0m                         device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     26\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m02d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  |  val RMSE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_rmse\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m val_rmse \u001B[38;5;241m<\u001B[39m best_rmse \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1e-4\u001B[39m:\n",
      "Cell \u001B[0;32mIn[9], line 11\u001B[0m, in \u001B[0;36mepoch_step\u001B[0;34m(model, optimizer, criterion, scheduler, train_loader, val_loader, device)\u001B[0m\n\u001B[1;32m      9\u001B[0m   optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     10\u001B[0m   loss \u001B[38;5;241m=\u001B[39m criterion(model(X), y)\n\u001B[0;32m---> 11\u001B[0m   loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     12\u001B[0m   optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    628\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    824\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    825\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train SlidingWindow",
   "id": "4c0f4229a8310536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.182068Z",
     "start_time": "2025-05-22T13:18:14.669036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert df_scaled to pytorch Tensor\n",
    "dataset, _ = tuds.make_panel_slidingwindows(data=df_scaled, features=FEATURES, target=TARGET, seq_len=SEQ_LEN,\n",
    "                                            horizon=HORIZON, keep_frac=KEEP_FRAC)"
   ],
   "id": "903cea0a30ba3521",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape original data:  (1099125, 32)\n",
      "Shape sampled:  (1099125, 32)\n",
      "Shape remainder:  (0, 32)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split dataset",
   "id": "3aa4275048371745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.182128Z",
     "start_time": "2025-05-22T13:18:21.855038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoaders for the 3 sets\n",
    "train_loader = DataLoader(\n",
    "  Subset(dataset, train_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  Subset(dataset, val_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "  Subset(dataset, test_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_workers=4\n",
    ")"
   ],
   "id": "7b60eb10eedac247",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model",
   "id": "7cc7538d509e4d3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.182191Z",
     "start_time": "2025-05-22T13:18:21.885717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save config\n",
    "SAVE_ENABLED = False\n",
    "SERIAL_NUMBER = f\"BasicLSTM-SlidingWindow{SEQ_LEN}\""
   ],
   "id": "5d29786c2ce4b0ab",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.182252Z",
     "start_time": "2025-05-22T13:18:21.890565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save best train iteration\n",
    "best_fold_state = None\n",
    "best_fold_rmse = float(\"inf\")"
   ],
   "id": "51be0a6b49769fd5",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:24:10.182301Z",
     "start_time": "2025-05-22T13:18:21.922696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset))), 1):\n",
    "  print(f\"=== FOLD {fold}/{N_SPLITS} ===\")\n",
    "  model = tumod.BasicLSTM(\n",
    "    n_features=len(FEATURES),\n",
    "    n_layers=N_LAYERS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    horizon=HORIZON).to(device=device)\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=PATIENCE\n",
    "  )\n",
    "\n",
    "  fold_rmse, best_state = fold_step(fold=fold,\n",
    "                                    train_idx=train_idx,\n",
    "                                    val_idx=val_idx,\n",
    "                                    dataset=dataset,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_epochs=NUM_EPOCHS,\n",
    "                                    patience=PATIENCE,\n",
    "                                    model=model,\n",
    "                                    device=device,\n",
    "                                    optimizer=optimizer,\n",
    "                                    criterion=criterion,\n",
    "                                    scheduler=scheduler)\n",
    "  if fold_rmse < best_fold_rmse:\n",
    "    best_fold_rmse = fold_rmse\n",
    "    best_fold_state = copy.deepcopy(best_state)"
   ],
   "id": "28d3e3dc50b4a3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD 1/3 ===\n",
      "Epoch 01/20  |  val RMSE: 2.5108\n",
      "Epoch 02/20  |  val RMSE: 2.3282\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE, weight_decay\u001B[38;5;241m=\u001B[39mWEIGHT_DECAY)\n\u001B[1;32m     11\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mReduceLROnPlateau(\n\u001B[1;32m     12\u001B[0m   optimizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, patience\u001B[38;5;241m=\u001B[39mPATIENCE\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m fold_rmse, best_state \u001B[38;5;241m=\u001B[39m fold_step(fold\u001B[38;5;241m=\u001B[39mfold,\n\u001B[1;32m     16\u001B[0m                                   train_idx\u001B[38;5;241m=\u001B[39mtrain_idx,\n\u001B[1;32m     17\u001B[0m                                   val_idx\u001B[38;5;241m=\u001B[39mval_idx,\n\u001B[1;32m     18\u001B[0m                                   dataset\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[1;32m     19\u001B[0m                                   batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE,\n\u001B[1;32m     20\u001B[0m                                   num_epochs\u001B[38;5;241m=\u001B[39mNUM_EPOCHS,\n\u001B[1;32m     21\u001B[0m                                   patience\u001B[38;5;241m=\u001B[39mPATIENCE,\n\u001B[1;32m     22\u001B[0m                                   model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     23\u001B[0m                                   device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     24\u001B[0m                                   optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[1;32m     25\u001B[0m                                   criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[1;32m     26\u001B[0m                                   scheduler\u001B[38;5;241m=\u001B[39mscheduler)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fold_rmse \u001B[38;5;241m<\u001B[39m best_fold_rmse:\n\u001B[1;32m     28\u001B[0m   best_fold_rmse \u001B[38;5;241m=\u001B[39m fold_rmse\n",
      "Cell \u001B[0;32mIn[26], line 23\u001B[0m, in \u001B[0;36mfold_step\u001B[0;34m(fold, train_idx, val_idx, dataset, batch_size, num_epochs, patience, model, device, optimizer, criterion, scheduler)\u001B[0m\n\u001B[1;32m     20\u001B[0m patience_left \u001B[38;5;241m=\u001B[39m patience\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 23\u001B[0m   val_rmse \u001B[38;5;241m=\u001B[39m epoch_step(model\u001B[38;5;241m=\u001B[39mmodel, optimizer\u001B[38;5;241m=\u001B[39moptimizer, criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[1;32m     24\u001B[0m                         scheduler\u001B[38;5;241m=\u001B[39mscheduler, train_loader\u001B[38;5;241m=\u001B[39mtrain_loader, val_loader\u001B[38;5;241m=\u001B[39mval_loader,\n\u001B[1;32m     25\u001B[0m                         device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     26\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m02d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  |  val RMSE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_rmse\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m val_rmse \u001B[38;5;241m<\u001B[39m best_rmse \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1e-4\u001B[39m:\n",
      "Cell \u001B[0;32mIn[25], line 12\u001B[0m, in \u001B[0;36mepoch_step\u001B[0;34m(model, optimizer, criterion, scheduler, train_loader, val_loader, device)\u001B[0m\n\u001B[1;32m     10\u001B[0m   loss \u001B[38;5;241m=\u001B[39m criterion(model(X), y)\n\u001B[1;32m     11\u001B[0m   loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 12\u001B[0m   optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     15\u001B[0m val_losses \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    489\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    490\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m             )\n\u001B[0;32m--> 493\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/optim/adamw.py:243\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    230\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m cast(Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m], group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    232\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    233\u001B[0m         group,\n\u001B[1;32m    234\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    240\u001B[0m         state_steps,\n\u001B[1;32m    241\u001B[0m     )\n\u001B[0;32m--> 243\u001B[0m     adamw(\n\u001B[1;32m    244\u001B[0m         params_with_grad,\n\u001B[1;32m    245\u001B[0m         grads,\n\u001B[1;32m    246\u001B[0m         exp_avgs,\n\u001B[1;32m    247\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    248\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    249\u001B[0m         state_steps,\n\u001B[1;32m    250\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    251\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    252\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    253\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    254\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    255\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    256\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    257\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    258\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    259\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    260\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    261\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    262\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    263\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    264\u001B[0m     )\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/optim/adamw.py:875\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    872\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    873\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 875\u001B[0m func(\n\u001B[1;32m    876\u001B[0m     params,\n\u001B[1;32m    877\u001B[0m     grads,\n\u001B[1;32m    878\u001B[0m     exp_avgs,\n\u001B[1;32m    879\u001B[0m     exp_avg_sqs,\n\u001B[1;32m    880\u001B[0m     max_exp_avg_sqs,\n\u001B[1;32m    881\u001B[0m     state_steps,\n\u001B[1;32m    882\u001B[0m     amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    883\u001B[0m     beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    884\u001B[0m     beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    885\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    886\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    887\u001B[0m     eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    888\u001B[0m     maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    889\u001B[0m     capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    890\u001B[0m     differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    891\u001B[0m     grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    892\u001B[0m     found_inf\u001B[38;5;241m=\u001B[39mfound_inf,\n\u001B[1;32m    893\u001B[0m     has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    894\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/optim/adamw.py:477\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[1;32m    475\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 477\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    479\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    481\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
