{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# xLSTM, sLSTM, mLSTMs",
   "id": "a1d433415f0426aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation",
   "id": "e8067785cd8d7919"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import modules",
   "id": "1fe72819da94dd5d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:26.885462Z",
     "start_time": "2025-06-30T11:35:26.882222Z"
    }
   },
   "source": [
    "# Prediction using LSTM, GRU-LSTM, xLSTM\n",
    "import copy\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import KFold, GroupShuffleSplit\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import thesis_utils as tu\n",
    "import thesis_utils.datastruc as tuds\n",
    "import thesis_utils.models as tumod"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "4284e3ddbaa79d6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:26.901808Z",
     "start_time": "2025-06-30T11:35:26.898017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model parameters\n",
    "HORIZON = 1\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 32\n",
    "NUM_EPOCHS = 25\n",
    "HIDDEN_SIZE = 128\n",
    "N_LAYERS = 3\n",
    "DROPOUT = 0.3\n",
    "XLSTM_TYPE = \"X\"\n",
    "N_LAGS = 5\n",
    "\n",
    "# Train parameters\n",
    "TARGET = \"EXPORT_centered\"\n",
    "FEATURES = [\n",
    "  \"contig\", \"comlang_off\", \"colony\", \"smctry\",\n",
    "]\n",
    "N_SPLITS = 5\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 0.01\n",
    "RANDOM_SEED = 16\n",
    "SUBSAMPLE_ENABLED = False\n",
    "N_DYADS = 1000\n",
    "\n",
    "SANCTION_COLS = [\"arms\", \"military\", \"trade\", \"travel\", \"other\"]\n",
    "\n",
    "# Torch config\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = (\n",
    "  torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "  else torch.device(\"cpu\")\n",
    ")"
   ],
   "id": "56e9b02ef240fd0a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "c9e963c3ab4f2220"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:27.342854Z",
     "start_time": "2025-06-30T11:35:26.904348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed = pd.read_parquet(path=\"../../data/model/processed.parquet\", engine=\"fastparquet\")\n",
    "df: DataFrame = processed.copy(deep=True)"
   ],
   "id": "9451e98a113f268d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sort, shift and compute data",
   "id": "84523341884506e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:27.734547Z",
     "start_time": "2025-06-30T11:35:27.352887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sort data by Report + Partner + Year\n",
    "df[\"dyad_id\"] = df[\"ISO3_reporter\"] + \"_\" + df[\"ISO3_partner\"]\n",
    "df = df.sort_values(by=[\"dyad_id\", \"Year\"], ignore_index=True)"
   ],
   "id": "1dcefe0333d9c64b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:27.788040Z",
     "start_time": "2025-06-30T11:35:27.746367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if SUBSAMPLE_ENABLED:\n",
    "  dyad_subsample = pd.Series(df[\"dyad_id\"].unique()).sample(n=N_DYADS, random_state=RANDOM_SEED, replace=False)\n",
    "  df = df[df[\"dyad_id\"].isin(dyad_subsample)]\n",
    "print(df[\"dyad_id\"].nunique())"
   ],
   "id": "37ec201db709a960",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33672\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:27.885990Z",
     "start_time": "2025-06-30T11:35:27.798658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"sanction\"] = (df[SANCTION_COLS]\n",
    "                  .sum(axis=1)).astype(int)"
   ],
   "id": "3fccd13d0f035fe6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Coerce numerical values and convert dyad_id to categorical",
   "id": "7a9d823465c895"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.089975Z",
     "start_time": "2025-06-30T11:35:27.895519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_cols = [\"distw\", \"GDP_reporter\", \"GDP_partner\", \"sanction\", \"contig\",\n",
    "            \"comlang_off\", \"colony\", \"smctry\", \"Year\", ]\n",
    "df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\").astype(float)\n",
    "df = df.dropna(subset=num_cols)"
   ],
   "id": "213988af4aee1df1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.192887Z",
     "start_time": "2025-06-30T11:35:28.100854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "for col in [\"dyad_id\"]:\n",
    "  df[col] = pd.Categorical(df[col], categories=sorted(df[col].unique()))"
   ],
   "id": "c56872ce3b8a13d9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Center data",
   "id": "d5984693829da5ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.227666Z",
     "start_time": "2025-06-30T11:35:28.202537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "center_columns = [\"distw\", \"GDP_reporter\", \"GDP_partner\", \"EXPORT\"]\n",
    "for col in center_columns:\n",
    "  median = df[col].median()\n",
    "  std_df = df[col].std()\n",
    "  df[col + \"_centered\"] = (df[col] - median) / std_df\n",
    "FEATURES += [\"distw_centered\"]"
   ],
   "id": "aeb1d300d2098b4f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.485123Z",
     "start_time": "2025-06-30T11:35:28.237908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lag_cols = [\"GDP_reporter_centered\", \"GDP_partner_centered\", \"sanction\"]\n",
    "for col in lag_cols:\n",
    "  for index in range(1, N_LAGS + 1):\n",
    "    df[f\"{col}_lag{index}\"] = df.groupby(\"dyad_id\", observed=True)[col].shift(index)"
   ],
   "id": "103ecd001ea6d514",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.820053Z",
     "start_time": "2025-06-30T11:35:28.495786Z"
    }
   },
   "cell_type": "code",
   "source": "df = df.dropna()",
   "id": "7a157c8465b5f5e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.833236Z",
     "start_time": "2025-06-30T11:35:28.831067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FEATURES += [f\"{c}_lag{index}\" for c in lag_cols for index in range(1, N_LAGS + 1)]\n",
    "# FEATURES += lag_cols"
   ],
   "id": "b2dd763dcc7fe017",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:28.847337Z",
     "start_time": "2025-06-30T11:35:28.843591Z"
    }
   },
   "cell_type": "code",
   "source": "FEATURES",
   "id": "35c49454d16ca04b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contig',\n",
       " 'comlang_off',\n",
       " 'colony',\n",
       " 'smctry',\n",
       " 'distw_centered',\n",
       " 'GDP_reporter_centered_lag1',\n",
       " 'GDP_reporter_centered_lag2',\n",
       " 'GDP_reporter_centered_lag3',\n",
       " 'GDP_reporter_centered_lag4',\n",
       " 'GDP_reporter_centered_lag5',\n",
       " 'GDP_partner_centered_lag1',\n",
       " 'GDP_partner_centered_lag2',\n",
       " 'GDP_partner_centered_lag3',\n",
       " 'GDP_partner_centered_lag4',\n",
       " 'GDP_partner_centered_lag5',\n",
       " 'sanction_lag1',\n",
       " 'sanction_lag2',\n",
       " 'sanction_lag3',\n",
       " 'sanction_lag4',\n",
       " 'sanction_lag5']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:29.974040Z",
     "start_time": "2025-06-30T11:35:28.865670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split into Train, Validation and Test sets\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_idx, test_idx = next(gss.split(df, groups=df[\"dyad_id\"]))\n",
    "test_df = df.iloc[test_idx]\n",
    "train_df = df.iloc[train_idx]\n",
    "\n",
    "train_idx, val_idx = next(gss.split(train_df, groups=train_df[\"dyad_id\"]))\n",
    "val_df = train_df.iloc[val_idx]\n",
    "train_df = train_df.iloc[train_idx]"
   ],
   "id": "51433a3f014528c2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.054193Z",
     "start_time": "2025-06-30T11:35:29.994118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df.loc[:, FEATURES] = train_df.loc[:, FEATURES].astype(\n",
    "  \"float32\",\n",
    "  copy=False\n",
    ")"
   ],
   "id": "a5ada7424fc6f996",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train",
   "id": "1053e63e8a6ccb76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Fold and Epoch steps\n",
    "_For reusability_"
   ],
   "id": "78e3575a51038c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.075183Z",
     "start_time": "2025-06-30T11:35:30.073197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create KFold object\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)"
   ],
   "id": "8eb0c16ea3f9a15d",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.096894Z",
     "start_time": "2025-06-30T11:35:30.093623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define epoch step\n",
    "def epoch_step(model: nn.Module, optimizer: Optimizer, criterion: nn.Module,\n",
    "               scheduler: LRScheduler, train_loader: DataLoader, val_loader: DataLoader,\n",
    "               device: any) -> float:\n",
    "  model.train()\n",
    "  for X, y, di in train_loader:\n",
    "    X, y, di = map(lambda t: t.to(device, non_blocking=True), (X, y, di))\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X, di)\n",
    "\n",
    "    if not torch.isfinite(y_pred).all():\n",
    "      print(\"⚠️ NaN or Inf detected in y_pred — stopping here!\")\n",
    "      return float(\"inf\")  # or break\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    if not torch.isfinite(loss):\n",
    "      print(\"⚠️ loss is NaN or Inf!\")\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  val_losses = []\n",
    "  with (torch.no_grad()):\n",
    "    for X, y, di in val_loader:\n",
    "      X, y, di = map(lambda t: t.to(device, non_blocking=True), (X, y, di))\n",
    "      val_losses.append(criterion(model(X, di), y).item())\n",
    "\n",
    "  val_rmse = math.sqrt((sum(val_losses) / len(val_losses)))\n",
    "  scheduler.step(val_rmse)\n",
    "  return val_rmse"
   ],
   "id": "82612d122ae0a3ae",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.117937Z",
     "start_time": "2025-06-30T11:35:30.113675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define fold step\n",
    "def fold_step(fold: int, train_idx: List, val_idx: List,\n",
    "              dataset: Dataset, batch_size: int, num_epochs: int, patience: int,\n",
    "              model: nn.Module, device: any,\n",
    "              optimizer: Optimizer, criterion: nn.Module, scheduler: LRScheduler) -> (float, dict):\n",
    "  train_loader = DataLoader(\n",
    "    Subset(dataset, train_idx),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True\n",
    "  )\n",
    "\n",
    "  val_loader = DataLoader(\n",
    "    Subset(dataset, val_idx),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    pin_memory=False\n",
    "  )\n",
    "\n",
    "  best_state = copy.deepcopy(model.state_dict())\n",
    "  best_rmse = float(\"inf\")\n",
    "  patience_left = patience\n",
    "\n",
    "  print(f\"Start epoch train for fold {fold}\")\n",
    "  for epoch in range(num_epochs):\n",
    "    val_rmse = epoch_step(model=model, optimizer=optimizer, criterion=criterion,\n",
    "                          scheduler=scheduler, train_loader=train_loader, val_loader=val_loader,\n",
    "                          device=device)\n",
    "    print(f\"Epoch {epoch + 1:02d}/{num_epochs}  |  val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    if val_rmse < best_rmse - 1e-4:\n",
    "      best_rmse, patience_left = val_rmse, 10\n",
    "      best_state = model.state_dict()\n",
    "    else:\n",
    "      patience_left -= 1\n",
    "      if patience_left == 0:\n",
    "        print(\"Early stop.\")\n",
    "        break\n",
    "  print(\"Load state dict\")\n",
    "  model.load_state_dict(best_state)\n",
    "  model.eval()\n",
    "  preds, truth = [], []\n",
    "  with torch.no_grad():\n",
    "    for X, y, di in val_loader:\n",
    "      X, di = map(lambda t: t.to(device, non_blocking=True), (X, di))\n",
    "      preds.append(model(X, di).cpu())\n",
    "      truth.append(y)\n",
    "  preds = torch.cat(preds).numpy()\n",
    "  truth = torch.cat(truth).numpy()\n",
    "\n",
    "  rmse = tu.rmse(truth, preds)\n",
    "  mae = tu.mae(truth, preds)\n",
    "  rmae = tu.rmae(truth, preds)\n",
    "  pseudo_r2 = tu.pseudo_r2(truth, preds)\n",
    "  print(f\"Fold {fold}  RMSE {rmse:.4f} | MAE {mae:.4f} | R² {pseudo_r2:.4f} | RMAE {rmae:.4f}\")\n",
    "\n",
    "  return rmse, copy.deepcopy(best_state)\n"
   ],
   "id": "959f98487abc0673",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Raw dataset",
   "id": "13f3e7e01662654"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split dataset",
   "id": "ff153528228b1350"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.386746Z",
     "start_time": "2025-06-30T11:35:30.134559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert df_scaled to pytorch Tensor\n",
    "dataset, dyad_to_idx = tuds.make_panel_datasets_dyad(\n",
    "  data=df,\n",
    "  features=FEATURES,\n",
    "  target=TARGET,\n",
    "  horizon=HORIZON,\n",
    ")"
   ],
   "id": "e38f7e2869374fc8",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.410439Z",
     "start_time": "2025-06-30T11:35:30.407466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoaders for the 3 sets\n",
    "train_loader = DataLoader(\n",
    "  Subset(dataset, train_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  num_workers=10,\n",
    "  persistent_workers=True,\n",
    "  prefetch_factor=2,\n",
    "  pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  Subset(dataset, val_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_workers=10,\n",
    "  persistent_workers=True,\n",
    "  prefetch_factor=2,\n",
    "  pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "  Subset(dataset, test_idx),\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False,\n",
    "  num_workers=10,\n",
    "  persistent_workers=True,\n",
    "  prefetch_factor=2,\n",
    "  pin_memory=False\n",
    ")"
   ],
   "id": "14dd8fdd80d8f0aa",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model",
   "id": "d9e348e74bd91747"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.431553Z",
     "start_time": "2025-06-30T11:35:30.429273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save config\n",
    "SAVE_ENABLED = False\n",
    "SERIAL_NUMBER = f\"{XLSTM_TYPE}LSTM-{LEARNING_RATE}lr-{DROPOUT}d-{HIDDEN_SIZE}hs\"\n",
    "SERIAL_NUMBER = SERIAL_NUMBER.replace(\".\", \"_\")\n",
    "PATH_TO_FOLDER = \"../../models/\""
   ],
   "id": "1b837c71b6de51c1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:35:30.454106Z",
     "start_time": "2025-06-30T11:35:30.452263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save best train iteration\n",
    "best_fold_state = None\n",
    "best_fold_rmse = float(\"inf\")"
   ],
   "id": "5e6e1bf43de12079",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:59:12.183005Z",
     "start_time": "2025-06-30T11:35:30.474607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset))), 1):\n",
    "  model = tumod.DyadXLSTM(\n",
    "    n_features=len(FEATURES),\n",
    "    n_dyads=len(dyad_to_idx),\n",
    "    embed_dim=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    horizon=HORIZON,\n",
    "    type=XLSTM_TYPE,\n",
    "  ).to(device=device)\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=PATIENCE\n",
    "  )\n",
    "\n",
    "  print(f\"=== FOLD {fold}/{N_SPLITS} ===\")\n",
    "  fold_rmse, best_state = fold_step(fold=fold,\n",
    "                                    train_idx=train_idx,\n",
    "                                    val_idx=val_idx,\n",
    "                                    dataset=dataset,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_epochs=NUM_EPOCHS,\n",
    "                                    patience=PATIENCE,\n",
    "                                    model=model,\n",
    "                                    device=device,\n",
    "                                    optimizer=optimizer,\n",
    "                                    criterion=criterion,\n",
    "                                    scheduler=scheduler)\n",
    "  if fold_rmse < best_fold_rmse:\n",
    "    best_fold_rmse = fold_rmse\n",
    "    best_fold_state = copy.deepcopy(best_state)"
   ],
   "id": "46a31ee0542aa6ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD 1/5 ===\n",
      "Start epoch train for fold 1\n",
      "Epoch 01/25  |  val RMSE: 0.9967\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 02/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 03/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 04/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 05/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 06/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 07/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 08/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 09/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 10/25  |  val RMSE: inf\n",
      "⚠️ NaN or Inf detected in y_pred — stopping here!\n",
      "Epoch 11/25  |  val RMSE: inf\n",
      "Early stop.\n",
      "Load state dict\n",
      "Fold 1  RMSE nan | MAE nan | R² nan | RMAE nan\n",
      "=== FOLD 2/5 ===\n",
      "Start epoch train for fold 2\n",
      "Epoch 01/25  |  val RMSE: 0.9023\n",
      "Epoch 02/25  |  val RMSE: 0.7523\n",
      "Epoch 03/25  |  val RMSE: 0.6822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 19\u001B[0m\n\u001B[1;32m     14\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mReduceLROnPlateau(\n\u001B[1;32m     15\u001B[0m   optimizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, patience\u001B[38;5;241m=\u001B[39mPATIENCE\n\u001B[1;32m     16\u001B[0m )\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== FOLD \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mN_SPLITS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 19\u001B[0m fold_rmse, best_state \u001B[38;5;241m=\u001B[39m fold_step(fold\u001B[38;5;241m=\u001B[39mfold,\n\u001B[1;32m     20\u001B[0m                                   train_idx\u001B[38;5;241m=\u001B[39mtrain_idx,\n\u001B[1;32m     21\u001B[0m                                   val_idx\u001B[38;5;241m=\u001B[39mval_idx,\n\u001B[1;32m     22\u001B[0m                                   dataset\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[1;32m     23\u001B[0m                                   batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE,\n\u001B[1;32m     24\u001B[0m                                   num_epochs\u001B[38;5;241m=\u001B[39mNUM_EPOCHS,\n\u001B[1;32m     25\u001B[0m                                   patience\u001B[38;5;241m=\u001B[39mPATIENCE,\n\u001B[1;32m     26\u001B[0m                                   model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     27\u001B[0m                                   device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     28\u001B[0m                                   optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[1;32m     29\u001B[0m                                   criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[1;32m     30\u001B[0m                                   scheduler\u001B[38;5;241m=\u001B[39mscheduler)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fold_rmse \u001B[38;5;241m<\u001B[39m best_fold_rmse:\n\u001B[1;32m     32\u001B[0m   best_fold_rmse \u001B[38;5;241m=\u001B[39m fold_rmse\n",
      "Cell \u001B[0;32mIn[27], line 32\u001B[0m, in \u001B[0;36mfold_step\u001B[0;34m(fold, train_idx, val_idx, dataset, batch_size, num_epochs, patience, model, device, optimizer, criterion, scheduler)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStart epoch train for fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 32\u001B[0m   val_rmse \u001B[38;5;241m=\u001B[39m epoch_step(model\u001B[38;5;241m=\u001B[39mmodel, optimizer\u001B[38;5;241m=\u001B[39moptimizer, criterion\u001B[38;5;241m=\u001B[39mcriterion,\n\u001B[1;32m     33\u001B[0m                         scheduler\u001B[38;5;241m=\u001B[39mscheduler, train_loader\u001B[38;5;241m=\u001B[39mtrain_loader, val_loader\u001B[38;5;241m=\u001B[39mval_loader,\n\u001B[1;32m     34\u001B[0m                         device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     35\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m02d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  |  val RMSE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_rmse\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     37\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m val_rmse \u001B[38;5;241m<\u001B[39m best_rmse \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1e-4\u001B[39m:\n",
      "Cell \u001B[0;32mIn[26], line 20\u001B[0m, in \u001B[0;36mepoch_step\u001B[0;34m(model, optimizer, criterion, scheduler, train_loader, val_loader, device)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misfinite(loss):\n\u001B[1;32m     18\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ loss is NaN or Inf!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 20\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     22\u001B[0m clip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m)\n\u001B[1;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    628\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/thesis_env/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    824\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    825\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Model",
   "id": "bbbbd3967e1612e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T11:59:12.197288Z",
     "start_time": "2025-06-29T23:02:25.117660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "  \"model_state_dict\": best_fold_state,\n",
    "  \"model_hyperparams\": {\n",
    "    \"n_features\": len(FEATURES),\n",
    "    \"n_dyads\": len(dyad_to_idx),\n",
    "    \"embed_dim\": EMBEDDING_SIZE,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"n_layers\": N_LAYERS,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"horizon\": HORIZON,\n",
    "  },\n",
    "  \"dyad_to_idx\": dyad_to_idx,\n",
    "  \"feature_names\": FEATURES,\n",
    "}, PATH_TO_FOLDER + SERIAL_NUMBER + \".pt\")"
   ],
   "id": "c9946e7cb88aabf9",
   "outputs": [],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
